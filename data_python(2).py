# -*- coding: utf-8 -*-
"""data_python

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGK9qDi2lND_VzpFipvNl_gcJxiHcP_O

# Tic-Tac-Toe Game Analysis and AI Development

## Introduction

Tic-Tac-Toe is one of the most fundamental strategy games, yet it contains rich patterns that can teach us about game theory, decision making, and artificial intelligence. In this analysis, we'll explore a comprehensive dataset of Tic-Tac-Toe games to understand:

### What We Want to Discover:
1. **Who has the advantage?** - Does the first player (X) really have an edge?
2. **Are some positions more powerful?** - Is the center square as important as we think?
3. **What makes a winning strategy?** - Can we identify patterns in successful gameplay?
4. **Can we build an AI?** - Using our insights, can we create an intelligent opponent?

### Our Approach:
We'll analyze this step-by-step:
- First, understand what's in our data
- Then explore game outcomes and patterns
- Identify strategic insights
- Finally, build an AI that learns from the best moves

### The Dataset:
Our dataset contains every possible unique Tic-Tac-Toe game, with complete move sequences and outcomes. Each row represents one complete game with:
- The winner (X, O, or Draw)
- Every move made by both players in sequence
- Positions marked as Row-Col coordinates (0-0 to 2-2)

# Tic-Tac-Toe Board Reference

## How the Board Works

### Standard 3x3 Grid Layout
```
 1 | 2 | 3
-----------
 4 | 5 | 6
-----------
 7 | 8 | 9
```

### Position Coordinates (Row-Col Format)
```
0-1 | 0-2 | 0-3
----|-----|----
1-1 | 1-2 | 1-3
----|-----|----
2-1 | 2-2 | 2-3
```

### Example Game State
```
 X | O | X     →    Position 0-1: X
-----------           Position 0-2: O  
 O | X | O     →    Position 0-3: X
-----------           Position 1-1: O
 - | X | -     →    Position 1-2: X
                     Position 1-3: O
                     Position 2-1: Empty
                     Position 2-2: X
                     Position 2-3: Empty
```

## Dataset Column Format

In our dataset, moves are recorded as:
- **Move 0-X (Row-Col)**: First move by X player (e.g., "2-2" for center)
- **Move 1-O (Row-Col)**: First move by O player (e.g., "1-1" for top-left)
- **Move 2-X (Row-Col)**: Second move by X player
- And so on...

## Common Position Names
- **0-1**: Top-Left Corner
- **0-2**: Top Edge  
- **0-3**: Top-Right Corner
- **1-1**: Left Edge
- **1-2**: Center (most strategic)
- **1-3**: Right Edge
- **2-1**: Bottom-Left Corner
- **2-2**: Bottom Edge
- **2-3**: Bottom-Right Corner

## Winning Combinations
There are 8 ways to win:

**Rows:**
- 0-1, 0-2, 0-3 (Top row)
- 1-1, 1-2, 1-3 (Middle row)  
- 2-1, 2-2, 2-3 (Bottom row)

**Columns:**
- 0-1, 1-1, 2-1 (Left column)
- 0-2, 1-2, 2-2 (Middle column)
- 0-3, 1-3, 2-3 (Right column)

**Diagonals:**
- 0-1, 1-2, 2-3 (Main diagonal)
- 0-3, 1-2, 2-1 (Anti-diagonal)

## Game Rules
1. **X always goes first**
2. **Players alternate turns**
3. **First to get 3 in a row wins**
4. **If board fills up with no winner → Draw**
5. **Games last 5-9 moves** (minimum 5 for fastest win, maximum 9 for full board)

---
"""

import kagglehub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

print("Loading Tic-Tac-Toe dataset...")
path = kagglehub.dataset_download("anthonytherrien/tic-tac-toe-game-dataset")

import os
files = os.listdir(path)
csv_file = [f for f in files if f.endswith('.csv')][0]
df = pd.read_csv(os.path.join(path, csv_file))

# Basic information about our dataset
print(f"We have {len(df):,} games in our dataset")
print(f"Each game has {df.shape[1]} pieces of information")

print("\n Here's what the first few games look like:")
print(df.head())

print(f"\n Column names in our dataset:")
for i, col in enumerate(df.columns, 1):
    print(f"  {i:2d}. {col}")

"""
## What we're analyzing:
The most basic question: who actually wins more in Tic-Tac-Toe? Is it the first player (X), the second player (O), or do most games end in a draw?

## Why this is important:
This tells us if the game is balanced or if there's a fundamental advantage to going first. In game theory, this is called "first-move advantage." Understanding this helps us:
- Know if the game is fair
- Understand the baseline probability of winning
- Set expectations for our AI's performance

## What to expect:
If Tic-Tac-Toe is a perfectly balanced game, we might expect many draws. If X (first player) has an advantage, we should see more X wins.

## Deep analysis we'll perform:
1. **Basic outcome distribution** - Raw counts and percentages
2. **Visual comparisons** - Multiple chart types to see patterns
3. **Statistical significance** - Is the difference meaningful?
4. **Win rate analysis** - What's the actual probability of each outcome?
5. **Game balance assessment** - How balanced is Tic-Tac-Toe really?

## Concepts:
- **First-move advantage**: The statistical benefit of playing first
- **Win probability**: The likelihood of each outcome occurring
- **Game balance**: How fair the game is between players"""

# Basic outcome analysis
winner_counts = df['Winner'].value_counts()
winner_percentages = df['Winner'].value_counts(normalize=True) * 100

print(" GAME OUTCOMES - BASIC STATISTICS:")
print("=" * 50)
for outcome in winner_counts.index:
    count = winner_counts[outcome]
    percentage = winner_percentages[outcome]
    print(f"{outcome:4s}: {count:,} games ({percentage:.2f}%)")

total_games = len(df)
x_wins = winner_counts['X']
o_wins = winner_counts['O']
draws = winner_counts['-']

print(f"\nTotal games analyzed: {total_games:,}")
print(f"Decisive games (not draws): {x_wins + o_wins:,} ({((x_wins + o_wins)/total_games*100):.1f}%)")

fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

# Basic outcome distribution
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
bars1 = ax1.bar(winner_counts.index, winner_counts.values, color=colors, alpha=0.8)
ax1.set_title('Game Outcomes Distribution', fontsize=14, fontweight='bold')
ax1.set_xlabel('Winner')
ax1.set_ylabel('Number of Games')
ax1.grid(axis='y', alpha=0.3)

for bar, count in zip(bars1, winner_counts.values):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 1000,
             f'{count:,}', ha='center', va='bottom', fontweight='bold')

# Most popular first moves analysis
first_moves_raw = df['Move 1-X (Row-Col)'].value_counts().head(6)
ax2.bar(range(len(first_moves_raw)), first_moves_raw.values, color='lightcoral', alpha=0.8)
ax2.set_title('Most Popular First Moves by X', fontsize=14, fontweight='bold')
ax2.set_xlabel('Starting Position')
ax2.set_ylabel('Times Played')
ax2.set_xticks(range(len(first_moves_raw)))
ax2.set_xticklabels([f'{pos}\n({count:,})' for pos, count in first_moves_raw.items()], rotation=45)
ax2.grid(axis='y', alpha=0.3)

# Win rate comparison (excluding draws)
decisive_games = x_wins + o_wins
x_win_rate_decisive = (x_wins / decisive_games) * 100
o_win_rate_decisive = (o_wins / decisive_games) * 100

win_rates = [x_win_rate_decisive, o_win_rate_decisive]
players = ['X (First)', 'O (Second)']
bars3 = ax3.bar(players, win_rates, color=['#FF6B6B', '#4ECDC4'], alpha=0.8)
ax3.set_title('Win Rate in Decisive Games Only', fontsize=14, fontweight='bold')
ax3.set_ylabel('Win Rate (%)')
ax3.set_ylim(0, 100)
ax3.axhline(y=50, color='gray', linestyle='--', alpha=0.7, label='Perfect Balance (50%)')
ax3.grid(axis='y', alpha=0.3)
ax3.legend()

for bar, rate in zip(bars3, win_rates):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height + 1,
             f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')

# Game length distribution (quick preview for next step)
game_lengths = df.apply(lambda row: sum(1 for col in df.columns if 'Move' in col and row[col] != '---' and pd.notna(row[col])), axis=1)
length_counts = game_lengths.value_counts().sort_index()

bars4 = ax4.bar(length_counts.index, length_counts.values, color='skyblue', alpha=0.8)
ax4.set_title('Game Length Distribution (Preview)', fontsize=14, fontweight='bold')
ax4.set_xlabel('Number of Moves')
ax4.set_ylabel('Number of Games')
ax4.grid(axis='y', alpha=0.3)

total_games = len(df)
for bar, length in zip(bars4, length_counts.index):
    height = bar.get_height()
    pct = (height / total_games) * 100
    ax4.text(bar.get_x() + bar.get_width()/2., height + 500,
             f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)

plt.tight_layout()
plt.show()

plt.tight_layout()
plt.show()

# Statistical Analysis
print(f"\n STATISTICAL ANALYSIS:")
print("=" * 40)
print(f"First-move advantage: {((x_wins - o_wins) / total_games * 100):+.2f} percentage points")
print(f"X advantage in decisive games: {(x_win_rate_decisive - 50):+.2f} percentage points")

expected_per_outcome = total_games / 3
print(f"\nIf game was perfectly balanced (33.33% each):")
print(f"Expected X wins: {expected_per_outcome:,.0f}")
print(f"Actual X wins: {x_wins:,} (difference: {x_wins - expected_per_outcome:+,.0f})")
print(f"Expected O wins: {expected_per_outcome:,.0f}")
print(f"Actual O wins: {o_wins:,} (difference: {o_wins - expected_per_outcome:+,.0f})")
print(f"Expected Draws: {expected_per_outcome:,.0f}")
print(f"Actual Draws: {draws:,} (difference: {draws - expected_per_outcome:+,.0f})")

balance_score = abs(x_wins - o_wins) / total_games * 100
print(f"\n GAME BALANCE ASSESSMENT:")
print(f"Balance score: {balance_score:.2f}% (lower = more balanced)")
if balance_score < 5:
    balance_rating = "Very Balanced"
elif balance_score < 10:
    balance_rating = "Moderately Balanced"
else:
    balance_rating = "Imbalanced"
print(f"Rating: {balance_rating}")

outcomes_numeric = df['Winner'].map({'X': 1, 'O': -1, '-': 0})
print(f"\n OUTCOME PATTERNS:")
print(f"Mean outcome value: {outcomes_numeric.mean():.3f}")
print(f"(Positive = X advantage, Negative = O advantage, Zero = balanced)")
print(f"Standard deviation: {outcomes_numeric.std():.3f}")
print(f"(Higher = more varied outcomes)")

plt.figure(figsize=(12, 4))
counts, bins, patches = plt.hist(outcomes_numeric, bins=[-1.5, -0.5, 0.5, 1.5],
                                 alpha=0.8, edgecolor='black')

colors = ['#4ECDC4', '#45B7D1', '#FF6B6B']  # O Wins, Draws, X Wins
for patch, color in zip(patches, colors):
    patch.set_facecolor(color)

plt.xlabel('Outcome Value')
plt.ylabel('Number of Games')
plt.title('Distribution of Game Outcomes (Numerical Representation)')
plt.xticks([-1, 0, 1], ['O Wins (-1)', 'Draws (0)', 'X Wins (1)'])
plt.grid(axis='y', alpha=0.3)
plt.show()

"""## What we're exploring:
Everyone says "take the center square first" in Tic-Tac-Toe. But is this actually true? Let's analyze all possible first moves to see which starting position gives the best winning chances.

## The mathematics behind it:
- There are 9 possible first moves (positions 0-0 through 2-2)
- We'll calculate the win rate for each starting position
- **Win rate formula**: (Number of X wins from that position) / (Total games from that position) × 100
- We'll also look at draw rates and O win rates from each starting position

## Strategic theory:
- **Center position (1-1)**: Controls the most lines (4 total: 2 diagonals, 1 row, 1 column)
- **Corner positions**: Control 3 lines each (2 edges + 1 diagonal)
- **Edge positions**: Control only 2 lines each (1 row + 1 column)

## What we expect to find:
If conventional wisdom is correct, the center position (1-1) should have the highest win rate for X. But we'll also discover:
- Which positions are most/least popular among players
- Whether players actually follow optimal strategy
- How much difference the starting move really makes

## Deep analysis we'll perform:
1. **Position popularity** - Where do players actually start?
2. **Win rates by position** - Which starting moves are most effective?
3. **Strategic value mapping** - Visual representation of board position values
4. **Edge vs Corner vs Center comparison** - Group analysis of position types
"""

def get_position_name(pos_str):
    """Convert '1-1' to 'Center', '0-0' to 'Top-Left', etc."""
    if pd.isna(pos_str) or pos_str == '---':
        return 'Unknown'

    row, col = map(int, pos_str.split('-'))
    position_names = {
        (0,0): 'Top-Left',    (0,1): 'Top-Center',    (0,2): 'Top-Right',
        (1,0): 'Middle-Left', (1,1): 'CENTER',        (1,2): 'Middle-Right',
        (2,0): 'Bottom-Left', (2,1): 'Bottom-Center', (2,2): 'Bottom-Right'
    }
    return position_names.get((row, col), f'Row{row}-Col{col}')

def get_position_type(pos_str):
    """Classify position as Corner, Edge, or Center"""
    if pd.isna(pos_str) or pos_str == '---':
        return 'Unknown'

    row, col = map(int, pos_str.split('-'))
    if row == 1 and col == 1:
        return 'Center'
    elif (row == 0 or row == 2) and (col == 0 or col == 2):
        return 'Corner'
    else:
        return 'Edge'

df['First_Move_Name'] = df['Move 1-X (Row-Col)'].apply(get_position_name)
df['Position_Type'] = df['Move 1-X (Row-Col)'].apply(get_position_type)

print(" FIRST MOVE POPULARITY:")
print("=" * 35)
first_moves = df['First_Move_Name'].value_counts()
total_games = len(df)

for position, count in first_moves.items():
    if position != 'Unknown':
        percentage = (count / total_games) * 100
        print(f"{position:13s}: {count:,} games ({percentage:.1f}%)")

print(f"\n WIN RATES BY STARTING POSITION:")
print("=" * 55)
print(f"{'Position':<13} {'Games':<8} {'X Win%':<8} {'O Win%':<8} {'Draw%':<8} {'X Advantage'}")
print("-" * 55)

position_stats = []
for position in df['First_Move_Name'].unique():
    if position == 'Unknown':
        continue

    games_from_position = df[df['First_Move_Name'] == position]
    total_games_pos = len(games_from_position)

    x_wins = len(games_from_position[games_from_position['Winner'] == 'X'])
    o_wins = len(games_from_position[games_from_position['Winner'] == 'O'])
    draws = len(games_from_position[games_from_position['Winner'] == '-'])

    x_win_rate = (x_wins / total_games_pos) * 100
    o_win_rate = (o_wins / total_games_pos) * 100
    draw_rate = (draws / total_games_pos) * 100
    x_advantage = x_win_rate - o_win_rate

    position_stats.append({
        'Position': position,
        'Games': total_games_pos,
        'X_Wins': x_wins,
        'O_Wins': o_wins,
        'Draws': draws,
        'X_Win_Rate': x_win_rate,
        'O_Win_Rate': o_win_rate,
        'Draw_Rate': draw_rate,
        'X_Advantage': x_advantage
    })

    print(f"{position:<13} {total_games_pos:<8,} {x_win_rate:<8.1f} {o_win_rate:<8.1f} {draw_rate:<8.1f} {x_advantage:+7.1f}")

position_df = pd.DataFrame(position_stats)
position_df = position_df.sort_values('X_Win_Rate', ascending=False)

fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

colors = ['red' if pos == 'CENTER' else 'lightblue' for pos in position_df['Position']]
bars1 = ax1.bar(range(len(position_df)), position_df['X_Win_Rate'], color=colors, alpha=0.8)
ax1.set_title('X Win Rate by Starting Position', fontsize=14, fontweight='bold')
ax1.set_xlabel('Starting Position')
ax1.set_ylabel('X Win Rate (%)')
ax1.set_xticks(range(len(position_df)))
ax1.set_xticklabels(position_df['Position'], rotation=45, ha='right')
ax1.grid(axis='y', alpha=0.3)

for bar, rate in zip(bars1, position_df['X_Win_Rate']):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
             f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')

avg_win_rate = position_df['X_Win_Rate'].mean()
ax1.axhline(y=avg_win_rate, color='orange', linestyle='--', alpha=0.7,
            label=f'Average: {avg_win_rate:.1f}%')
ax1.legend()

# Position popularity vs effectiveness (cleaner version)
ax2.scatter(position_df['Games'], position_df['X_Win_Rate'],
           s=[300 if pos == 'CENTER' else 150 for pos in position_df['Position']],
           c=['red' if pos == 'CENTER' else 'blue' for pos in position_df['Position']],
           alpha=0.7, edgecolors='black', linewidth=1)
ax2.set_xlabel('Times Played')
ax2.set_ylabel('X Win Rate (%)')
ax2.set_title('Popularity vs Effectiveness of Starting Positions', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3)

label_offsets = {
    'CENTER': (10, 10),
    'Top-Left': (-15, -15),
    'Top-Right': (10, -15),
    'Bottom-Left': (-15, 10),
    'Bottom-Right': (10, 10),
    'Top-Center': (0, 15),
    'Bottom-Center': (0, -20),
    'Middle-Left': (-20, 0),
    'Middle-Right': (15, 0)
}

for _, row in position_df.iterrows():
    pos = row['Position']
    offset = label_offsets.get(pos, (5, 5))
    ax2.annotate(pos, (row['Games'], row['X_Win_Rate']),
                xytext=offset, textcoords='offset points',
                fontsize=10, fontweight='bold' if pos == 'CENTER' else 'normal',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow' if pos == 'CENTER' else 'white',
                         alpha=0.8, edgecolor='black'))

#Visual board heatmap of win rates
win_rate_grid = np.zeros((3, 3))
for _, row in position_df.iterrows():
    pos_name = row['Position']
    win_rate = row['X_Win_Rate']

    pos_map = {
        'Top-Left': (0,0), 'Top-Center': (0,1), 'Top-Right': (0,2),
        'Middle-Left': (1,0), 'CENTER': (1,1), 'Middle-Right': (1,2),
        'Bottom-Left': (2,0), 'Bottom-Center': (2,1), 'Bottom-Right': (2,2)
    }

    if pos_name in pos_map:
        r, c = pos_map[pos_name]
        win_rate_grid[r, c] = win_rate

im = ax3.imshow(win_rate_grid, cmap='RdYlBu_r', aspect='equal')
ax3.set_title('Board Position Win Rate Heatmap', fontsize=14, fontweight='bold')

for i in range(3):
    for j in range(3):
        text = ax3.text(j, i, f'{win_rate_grid[i, j]:.1f}%',
                       ha="center", va="center", color="black", fontweight='bold')

ax3.set_xticks([])
ax3.set_yticks([])
plt.colorbar(im, ax=ax3, label='X Win Rate (%)')

# Position type analysis (Corner vs Edge vs Center)
type_analysis = df.groupby('Position_Type').agg({
    'Winner': ['count', lambda x: (x == 'X').sum(), lambda x: (x == 'O').sum(), lambda x: (x == '-').sum()]
}).round(2)

type_analysis.columns = ['Total_Games', 'X_Wins', 'O_Wins', 'Draws']
type_analysis['X_Win_Rate'] = (type_analysis['X_Wins'] / type_analysis['Total_Games']) * 100

if 'Unknown' in type_analysis.index:
    type_analysis = type_analysis.drop('Unknown')

bars4 = ax4.bar(type_analysis.index, type_analysis['X_Win_Rate'],
               color=['gold', 'lightcoral', 'lightgreen'], alpha=0.8)
ax4.set_title('X Win Rate by Position Type', fontsize=14, fontweight='bold')
ax4.set_ylabel('X Win Rate (%)')
ax4.grid(axis='y', alpha=0.3)

for bar, pos_type in zip(bars4, type_analysis.index):
    height = bar.get_height()
    games = type_analysis.loc[pos_type, 'Total_Games']
    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,
             f'{height:.1f}%\n({games:,} games)', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

# Key insights analysis
center_stats = position_df[position_df['Position'] == 'CENTER'].iloc[0]
best_position = position_df.iloc[0]
worst_position = position_df.iloc[-1]

print(f"\n CENTER SQUARE ANALYSIS:")
print("=" * 40)
print(f"Center win rate: {center_stats['X_Win_Rate']:.2f}%")
print(f"Center rank: #{position_df[position_df['Position'] == 'CENTER'].index[0] + 1} out of 9")
print(f"Best position: {best_position['Position']} ({best_position['X_Win_Rate']:.2f}%)")
print(f"Worst position: {worst_position['Position']} ({worst_position['X_Win_Rate']:.2f}%)")
print(f"Center vs Best: {center_stats['X_Win_Rate'] - best_position['X_Win_Rate']:+.2f} percentage points")
print(f"Center vs Average: {center_stats['X_Win_Rate'] - avg_win_rate:+.2f} percentage points")

print(f"\n POSITION TYPE COMPARISON:")
print("=" * 35)
for pos_type in type_analysis.index:
    win_rate = type_analysis.loc[pos_type, 'X_Win_Rate']
    games = type_analysis.loc[pos_type, 'Total_Games']
    print(f"{pos_type:<8}: {win_rate:.2f}% win rate ({games:,} games)")

"""Key Insights from Our Analysis:
 THE CENTER IS THE BEST CHOICE!

Center square has the highest win rate: 60.5%
This means if you start with center, you win 6 out of every 10 games
Much better than the overall average of 51.6%

 CORNERS ARE SOLID SECOND CHOICE:

All 4 corners have the same 52.8% win rate
This means you win about 5 out of every 10 games
All corners perform equally well - no difference between them

❌ EDGES ARE THE WORST CHOICE:

Edge positions only have 48.1% win rate
This means you win less than 5 out of every 10 games
You're actually more likely to lose or draw than win

 SURPRISING PLAYER BEHAVIOR:

Center is only the 5th most popular choice
Most players choose edge positions more often
This shows many players don't know the best strategy

 OPTIMAL STRATEGY:

Always choose CENTER first (60.5% chance to win)
If center taken, choose any CORNER (52.8% chance to win)
Avoid EDGES (only 48.1% chance to win)

 THE DIFFERENCE IS HUGE:

Center vs Corner: 60.5% vs 52.8% = 7.7% better
Corner vs Edge: 52.8% vs 48.1% = 4.7% better
Center vs Edge: 60.5% vs 48.1% = 12.4% better

 BOTTOM LINE:
The old advice is completely right! Center first, then corners, never edges!

# Step 4: Game Length and Strategy Patterns

## What we're investigating:
How long do Tic-Tac-Toe games typically last? Do quick games favor one player over another? This can reveal strategic patterns and help us understand different playing styles.

## The logic behind game length:
- **Minimum game length**: 5 moves (X wins in just 3 moves - fastest possible win)
- **Maximum game length**: 9 moves (board completely full, must be a draw)
- **Possible lengths**: Only 5, 6, 7, 8, or 9 moves (no other possibilities!)
- **Quick wins (5-6 moves)**: Suggest superior strategy or opponent mistakes
- **Longer games (7-9 moves)**: Indicate more defensive, careful play from both sides

## What this tells us about strategy:
- If X wins more in shorter games, it suggests first-move advantage is strongest with aggressive play
- If games often go to 8-9 moves, it means players are good at blocking each other
- The distribution of game lengths shows how often games are "competitive" vs "one-sided"

## Analysis we'll perform:
1. **Game length distribution** - How many moves do most games take?
2. **Winner by game length** - Who wins in quick vs long games?
3. **Strategic implications** - What do different game lengths mean?
4. **Defensive vs aggressive patterns** - How often do players successfully defend?

## Why this matters for AI:
Understanding game length patterns helps us:
- Know when games are typically decided
- Identify critical decision points
- Understand the importance of early moves vs late game
- Prepare our AI for different game scenarios
"""

game_lengths = df.apply(lambda row: sum(1 for col in df.columns if 'Move' in col and row[col] != '---' and pd.notna(row[col])), axis=1)
df['Game_Length'] = game_lengths

unique_lengths = sorted(df['Game_Length'].unique())
print(f" VERIFICATION - Game lengths found: {unique_lengths}")

print(f"\n  GAME LENGTH DISTRIBUTION:")
print("=" * 40)

length_counts = df['Game_Length'].value_counts().sort_index()
total_games = len(df)

for length in length_counts.index:
    count = length_counts[length]
    percentage = (count / total_games) * 100
    print(f"{length} moves: {count:,} games ({percentage:.1f}%)")

print(f"\n WHO WINS AT DIFFERENT GAME LENGTHS:")
print("=" * 55)
print(f"{'Length':<8} {'Total':<8} {'X Wins':<8} {'O Wins':<8} {'Draws':<8} {'X Win%':<8}")
print("-" * 55)

length_analysis = []
for length in sorted(df['Game_Length'].unique()):
    games_of_length = df[df['Game_Length'] == length]
    total = len(games_of_length)
    x_wins = len(games_of_length[games_of_length['Winner'] == 'X'])
    o_wins = len(games_of_length[games_of_length['Winner'] == 'O'])
    draws = len(games_of_length[games_of_length['Winner'] == '-'])
    x_win_rate = (x_wins / total) * 100 if total > 0 else 0

    length_analysis.append({
        'Length': length,
        'Total': total,
        'X_Wins': x_wins,
        'O_Wins': o_wins,
        'Draws': draws,
        'X_Win_Rate': x_win_rate
    })

    print(f"{length:<8} {total:<8,} {x_wins:<8,} {o_wins:<8,} {draws:<8,} {x_win_rate:<8.1f}")

fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

# Game length distribution
bars1 = ax1.bar(length_counts.index, length_counts.values, color='skyblue', alpha=0.8)
ax1.set_title('How Long Do Games Last?', fontsize=14, fontweight='bold')
ax1.set_xlabel('Number of Moves')
ax1.set_ylabel('Number of Games')
ax1.grid(axis='y', alpha=0.3)

for length, count in length_counts.items():
    pct = (count / total_games) * 100
    ax1.text(length, count + 2000, f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold')

# X win rate by game length
length_df = pd.DataFrame(length_analysis)
bars2 = ax2.bar(length_df['Length'], length_df['X_Win_Rate'],
               color=['red' if x < 50 else 'green' for x in length_df['X_Win_Rate']], alpha=0.8)
ax2.set_title('X Win Rate by Game Length', fontsize=14, fontweight='bold')
ax2.set_xlabel('Game Length (moves)')
ax2.set_ylabel('X Win Rate (%)')
ax2.axhline(y=50, color='gray', linestyle='--', alpha=0.7, label='50% (Balanced)')
ax2.grid(axis='y', alpha=0.3)
ax2.legend()

for i, (bar, row) in enumerate(zip(bars2, length_df.itertuples())):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
             f'{row.X_Win_Rate:.1f}%\n({row.Total:,} games)',
             ha='center', va='bottom', fontweight='bold', fontsize=9)

length_winner_data = df.groupby(['Game_Length', 'Winner']).size().unstack(fill_value=0)
column_order = ['X', 'O', '-']
length_winner_data = length_winner_data[column_order]

length_winner_data.plot(kind='bar', stacked=True, ax=ax3,
                       color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)
ax3.set_title('Game Outcomes by Length (Absolute Numbers)', fontsize=14, fontweight='bold')
ax3.set_xlabel('Game Length (moves)')
ax3.set_ylabel('Number of Games')
ax3.legend(title='Winner', labels=['X Wins', 'O Wins', 'Draws'])
ax3.tick_params(axis='x', rotation=0)

# Percentage distribution by game length
length_winner_pct = length_winner_data.div(length_winner_data.sum(axis=1), axis=0) * 100

length_winner_pct.plot(kind='bar', stacked=True, ax=ax4,
                      color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)
ax4.set_title('Game Outcomes by Length (Percentages)', fontsize=14, fontweight='bold')
ax4.set_xlabel('Game Length (moves)')
ax4.set_ylabel('Percentage')
ax4.legend(title='Winner', labels=['X Wins', 'O Wins', 'Draws'])
ax4.tick_params(axis='x', rotation=0)

plt.tight_layout()
plt.show()

print(f"\n GAME LENGTH INSIGHTS:")
print("=" * 35)

shortest_games = df[df['Game_Length'] == df['Game_Length'].min()]
longest_games = df[df['Game_Length'] == df['Game_Length'].max()]
most_common_length = length_counts.idxmax()

print(f"Shortest possible game: {df['Game_Length'].min()} moves")
print(f"Longest possible game: {df['Game_Length'].max()} moves")
print(f"Most common game length: {most_common_length} moves ({length_counts[most_common_length]:,} games)")
print(f"Average game length: {df['Game_Length'].mean():.1f} moves")

# Quick vs slow games analysis
quick_games = df[df['Game_Length'] <= 6]  # Games ending in 6 moves or less
slow_games = df[df['Game_Length'] >= 8]   # Games lasting 8+ moves

quick_x_rate = (quick_games['Winner'] == 'X').mean() * 100
slow_x_rate = (slow_games['Winner'] == 'X').mean() * 100

print(f"\n QUICK vs SLOW GAMES:")
print("=" * 30)
print(f"Quick games (≤6 moves): {len(quick_games):,} games")
print(f"  X win rate: {quick_x_rate:.1f}%")
print(f"Slow games (≥8 moves): {len(slow_games):,} games")
print(f"  X win rate: {slow_x_rate:.1f}%")
print(f"Difference: {quick_x_rate - slow_x_rate:+.1f}% (positive means X better in quick games)")

# Strategic implications
print(f"\n STRATEGIC IMPLICATIONS:")
print("=" * 35)

best_length_for_x = length_df.loc[length_df['X_Win_Rate'].idxmax()]
worst_length_for_x = length_df.loc[length_df['X_Win_Rate'].idxmin()]

print(f"X's best performance: {best_length_for_x['Length']} moves ({best_length_for_x['X_Win_Rate']:.1f}% win rate)")
print(f"X's worst performance: {worst_length_for_x['Length']} moves ({worst_length_for_x['X_Win_Rate']:.1f}% win rate)")

if 9 in length_counts.index:
    full_games = df[df['Game_Length'] == 9]
    full_game_outcomes = full_games['Winner'].value_counts()
    print(f"\nFull board games (9 moves): {len(full_games):,}")
    for outcome, count in full_game_outcomes.items():
        pct = (count / len(full_games)) * 100
        print(f"  {outcome}: {count:,} ({pct:.1f}%)")

"""Key Insights from Our Analysis:
 MOST GAMES GO THE DISTANCE

Half of all games (50.1%) last 10 moves - the maximum possible
Only 2.7% of games end quickly (6-7 moves)
This shows most players are decent at defense and blocking wins

 QUICK GAMES HAVE EXTREME OUTCOMES

6-move games: X wins 100% of the time (1,440 games)
7-move games: O wins 100% of the time (5,328 games)
8-move games: X wins 100% of the time (47,952 games)
9-move games: O wins 100% of the time (72,576 games)

 THE PATTERN IS CRYSTAL CLEAR

Even-numbered moves (6, 8): X always wins
Odd-numbered moves (7, 9): O always wins
10-move games: Mixed outcomes (64% X wins, 36% draws)

 WHAT THIS REVEALS ABOUT STRATEGY

Quick wins mean mistakes: When games end early, one player made a major error
Most players defend well: 78.5% of games reach 9-10 moves
First-move advantage is real: X wins 64% of full-length games
No draws in quick games: Early endings are always decisive

 SURPRISING DISCOVERIES

Zero draws until 10 moves: Quick games never end in ties
Perfect alternating pattern: Win rates flip completely between even/odd lengths
Defensive play is common: Most games are competitive battles, not blowouts

 BOTTOM LINE
Most Tic-Tac-Toe games are well-fought battles that go nearly full length. Quick games happen when someone makes a serious mistake, and X's first-move advantage only really shows up in longer, more competitive games.

# Step 5: Machine Learning Predictions with Logistic Regression

## What we're building:
A simple machine learning model using **Logistic Regression** to predict who will win a Tic-Tac-Toe game. We'll start with the basics since we're beginners!

## Why Logistic Regression?
- **Simple to understand**: Makes predictions based on probabilities
- **Fast to train**: Works quickly even with large datasets
- **Good for beginners**: Easy to interpret results
- **Perfect for classification**: Predicts categories (X wins, O wins, Draw)

## The learning process:
1. **Training (70% of data)**: The model learns patterns from 178,982 games
2. **Testing (30% of data)**: We test on 76,706 "unseen" games to see how well it learned
3. **No cheating**: The model never sees the test data during training

## What the model learns:
- **Which opening moves lead to wins**: Does starting in center really help?
- **Move sequences that matter**: How do early moves affect the outcome?
- **Game length patterns**: Do longer games favor X or O?

## How we measure success:
- **Accuracy**: What percentage of predictions are correct?
- **F1 Score**: Balances precision and recall (important for imbalanced data)
- **Confusion Matrix**: Shows exactly where the model makes mistakes
- **Comparison plots**: Visual comparison between predictions and reality

## What makes a good model:
- **Random guessing**: 33% accuracy (3 possible outcomes)
- **Decent model**: 60-70% accuracy
- **Good model**: 70-80% accuracy  
- **Excellent model**: 80%+ accuracy

## Features we'll use:
- **All moves made**: Every X and O move in sequence
- **Game length**: How many total moves were played
- **Move positions**: Which squares were chosen
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.preprocessing import LabelEncoder
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

print(" STEP 5: MACHINE LEARNING WITH LOGISTIC REGRESSION")
print("=" * 60)
print("We're starting simple! Logistic Regression is perfect for beginners.")
print("It's easy to understand and works great for classification problems.")

move_columns = [col for col in df.columns if 'Move' in col and 'Row-Col' in col]
print(f" Found {len(move_columns)} move columns to use as features")
print(f"Features are like 'clues' the model uses to make predictions")

print(f"\n Converting text moves to numbers...")
le_dict = {}  # Store our encoders
X_encoded = pd.DataFrame()

for col in move_columns:
    le = LabelEncoder()
    unique_values = df[col].unique()
    le.fit(unique_values)
    le_dict[col] = le

    X_encoded[col] = le.transform(df[col])

X_encoded['Game_Length'] = df['Game_Length']
print(f" Encoded {len(move_columns)} move features + game length")

y = df['Winner']
print(f"\n TARGET VARIABLE (what we're predicting):")
target_counts = y.value_counts()
for outcome, count in target_counts.items():
    percentage = (count / len(y)) * 100
    print(f"  {outcome}: {count:,} games ({percentage:.1f}%)")

print(f"\n STEP 2: SPLITTING DATA (70% TRAIN, 30% TEST)")

X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\n DATA SPLIT COMPLETED:")
print(f" Training set: {len(X_train):,} games ({len(X_train)/len(df)*100:.1f}%)")
print(f" Testing set: {len(X_test):,} games ({len(X_test)/len(df)*100:.1f}%)")
print(f" Total check: {len(X_train) + len(X_test):,} = {len(df):,} ✅")

print(f"\n STEP 3: TRAINING THE LOGISTIC REGRESSION MODEL")


from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
weight_dict = dict(zip(np.unique(y_train), class_weights))

weight_dict['-'] = weight_dict['-'] * 0.6  # Reduce draw weight by 40%


model = LogisticRegression(random_state=42, max_iter=2000, class_weight=weight_dict)
print(f" Training model on {len(X_train):,} games...")

model.fit(X_train, y_train)
print(f" Model trained successfully!")

print(f"\n STEP 4: MAKING PREDICTIONS ON TEST DATA")

y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)

print(f" Checking prediction distribution...")
pred_distribution = pd.Series(y_pred).value_counts()
print(f"Predictions made: {pred_distribution.to_dict()}")

if '-' not in pred_distribution.index:
    print(f" Model didn't predict any draws! Let's try adjusting...")

    # Get the probabilities for each class
    class_labels = model.classes_

    # Create custom predictions where we predict draw if no class has >60% confidence
    max_proba = np.max(y_pred_proba, axis=1)
    confidence_threshold = 0.6

    # Alternative predictions with confidence threshold
    y_pred_adjusted = y_pred.copy()
    low_confidence_mask = max_proba < confidence_threshold

    if low_confidence_mask.sum() > 0:
        # For low confidence predictions, let's see if any should be draws
        print(f" Found {low_confidence_mask.sum()} low-confidence predictions")
        # This is a simple heuristic - in practice you'd want more sophisticated methods

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

print(f" Made predictions for {len(y_test):,} test games")
print(f" Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)")
print(f" F1 Score: {f1:.3f} ({f1*100:.1f}%)")

# Compare with random guessing
random_accuracy = 1/3
improvement = ((accuracy - random_accuracy) / random_accuracy) * 100
print(f"\n📈 PERFORMANCE VS RANDOM GUESSING:")
print(f"Random guessing: {random_accuracy:.3f} (33.3%)")
print(f"Our model: {accuracy:.3f} ({accuracy*100:.1f}%)")
print(f"Improvement: {improvement:.1f}% better than random!")

print(f"\n STEP 5: DETAILED ANALYSIS & VISUALIZATIONS")
print("-" * 50)

fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Logistic Regression Model Analysis', fontsize=16, fontweight='bold')

#  Actual vs Predicted Distribution
all_categories = ['X', 'O', '-']
actual_counts = pd.Series(y_test).value_counts().reindex(all_categories, fill_value=0)
predicted_counts = pd.Series(y_pred).value_counts().reindex(all_categories, fill_value=0)

x_pos = np.arange(len(all_categories))
width = 0.35

bars1 = ax1.bar(x_pos - width/2, actual_counts.values, width,
                label='Actual', color='lightblue', alpha=0.8)
bars2 = ax1.bar(x_pos + width/2, predicted_counts.values, width,
                label='Predicted', color='lightcoral', alpha=0.8)

ax1.set_title('Actual vs Predicted Distribution', fontsize=14, fontweight='bold')
ax1.set_xlabel('Winner')
ax1.set_ylabel('Number of Games')
ax1.set_xticks(x_pos)
ax1.set_xticklabels(['X Wins', 'O Wins', 'Draws'])
ax1.legend()
ax1.grid(axis='y', alpha=0.3)

for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 50,
                 f'{int(height):,}', ha='center', va='bottom', fontweight='bold')

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=['X', 'O', '-'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,
            xticklabels=['X', 'O', 'Draw'], yticklabels=['X', 'O', 'Draw'])
ax2.set_title('Confusion Matrix\n(Where predictions go wrong)', fontsize=14, fontweight='bold')
ax2.set_xlabel('Predicted')
ax2.set_ylabel('Actual')

outcomes = ['X', 'O', '-']
accuracies_by_outcome = []

for outcome in outcomes:
    mask = y_test == outcome
    if mask.sum() > 0:
        acc = (y_test[mask] == y_pred[mask]).mean()
        accuracies_by_outcome.append(acc)
    else:
        accuracies_by_outcome.append(0)

colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
bars3 = ax3.bar(outcomes, accuracies_by_outcome, color=colors, alpha=0.8)
ax3.set_title('Prediction Accuracy by Outcome', fontsize=14, fontweight='bold')
ax3.set_xlabel('Actual Winner')
ax3.set_ylabel('Prediction Accuracy')
ax3.set_ylim(0, 1)
ax3.grid(axis='y', alpha=0.3)

for bar, acc in zip(bars3, accuracies_by_outcome):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,
             f'{acc:.1%}', ha='center', va='bottom', fontweight='bold')

#Model Confidence Distribution
max_probabilities = np.max(y_pred_proba, axis=1)
ax4.hist(max_probabilities, bins=30, color='skyblue', alpha=0.8, edgecolor='black')
ax4.set_title('Model Confidence Distribution', fontsize=14, fontweight='bold')
ax4.set_xlabel('Maximum Predicted Probability')
ax4.set_ylabel('Number of Predictions')
ax4.grid(axis='y', alpha=0.3)
ax4.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='50% threshold')
ax4.legend()

plt.tight_layout()
plt.show()

# Detailed classification report for best model
print(f"\n DETAILED PERFORMANCE REPORT:")
print()
print(classification_report(y_test, y_pred, target_names=['Draw', 'O Wins', 'X Wins'], zero_division=0))

print(f"\n WHAT OUR MODEL LEARNED")

print(f"The model found patterns in the data and can now predict winners!")
print(f"Here's what makes our model good or bad:")
print()

if accuracy >= 0.80:
    grade = " EXCELLENT"
    comment = "The model learned very strong patterns!"
elif accuracy >= 0.70:
    grade = " GOOD"
    comment = "The model learned solid patterns."
elif accuracy >= 0.60:
    grade = " FAIR"
    comment = "The model learned some patterns but could improve."
else:
    grade = " NEEDS WORK"
    comment = "The model struggled to find clear patterns."

print(f"Overall Grade: {grade}")
print(f"Comment: {comment}")
print(f"Accuracy: {accuracy*100:.1f}% (Random guessing: 33.3%)")

# CONFUSION MATRIX ANALYSIS
print(f"\n CONFUSION MATRIX ANALYSIS")


total_correct = np.trace(cm)
total_predictions = cm.sum()
print(f" Correct predictions: {total_correct:,} out of {total_predictions:,}")

for i, outcome in enumerate(['X', 'O', 'Draw']):
    correct = cm[i, i]
    total_actual = cm[i, :].sum()
    accuracy_for_outcome = correct / total_actual if total_actual > 0 else 0
    print(f"{outcome} wins: {correct:,}/{total_actual:,} correct ({accuracy_for_outcome:.1%})")

print(f"\n BALANCING ACT:")
print(f"• No weights: Model ignores draws completely")
print(f"• Full balance: Model over-predicts draws")
print(f"• Custom weights: Better balance between all outcomes")
print(f"• Our approach: Give draws a boost but keep it realistic")

print(f"\n KEY TAKEAWAYS:")

print(f"• Our model is {improvement:.0f}% better than random guessing")
print(f"• It can predict Tic-Tac-Toe winners with {accuracy*100:.0f}% accuracy")
print(f"• Now predicts all three outcomes more realistically")
print(f"• Custom class weights help balance the predictions")
print(f"• Perfect example of real-world ML challenges!")

"""# Machine Learning Analysis Report: Tic-Tac-Toe Winner Prediction

## Executive Summary

We successfully built a logistic regression model to predict Tic-Tac-Toe game winners with **81.2% accuracy** - significantly outperforming random guessing (33.3%) by **144%**. However, the model reveals important insights about class imbalance in machine learning.

---

## Dataset Overview

- **Total Games**: 255,168
- **Training Set**: 178,617 games (70%)
- **Test Set**: 76,551 games (30%)
- **Features Used**: 9 move sequences + game length

### Class Distribution
- **X Wins**: 51.4% (131,184 games)
- **O Wins**: 30.5% (77,904 games)
- **Draws**: 18.1% (46,080 games)

---

## Model Performance

### Overall Results
- **Accuracy**: 81.2%
- **F1 Score**: 73.6%
- **Improvement over Random**: 144%

### Performance by Outcome
| Outcome | Precision | Recall | F1-Score | Support |
|---------|-----------|--------|----------|---------|
| **X Wins** | 74% | 99% | 84% | 39,356 |
| **O Wins** | 98% | 99% | 99% | 23,371 |
| **Draws** | 0% | 0% | 0% | 13,824 |

---

## Key Findings

###  What the Model Does Well
1. **Excellent at predicting O wins** (98% precision, 99% recall)
2. **Very good at predicting X wins** (74% precision, 99% recall)
3. **Strong overall accuracy** at 81.2%
4. **Learned real patterns** from move sequences

###  Critical Limitation
**The model completely fails to predict draws** (0% recall), treating all draws as either X or O wins.

###  Why This Happens
1. **Class Imbalance**: Draws are only 18% of data
2. **Safety Strategy**: Model learned it's "safer" to predict X or O wins
3. **Cost-Benefit**: Wrong X/O prediction vs missed draw has different penalties
4. **Pattern Complexity**: Draw patterns may be more complex to detect

---

## Technical Analysis


### Prediction Distribution
- **X Predictions**: 52,870 (69%)
- **O Predictions**: 23,681 (31%)
- **Draw Predictions**: 0 (0%)

### Confidence Analysis
- Model made 38,931 low-confidence predictions but still didn't predict draws
- High confidence in X vs O discrimination
- Struggle with draw identification


## Real-World Implications

### Business Context
If this were a real application:
- **Customer Satisfaction**: 81% accuracy is excellent
- **Risk Management**: Missing draws might be acceptable depending on use case
- **Cost Analysis**: Need to evaluate cost of false positives vs false negatives




## Conclusions

### What We Achieved
- Built a working ML model with 81% accuracy
- Demonstrated clear improvement over random guessing (144% better)
- Learned practical lessons about class imbalance
- Successfully implemented proper train/test methodology

### What We Learned
- **High accuracy doesn't guarantee good performance on all classes**
- **Class imbalance is a major challenge in real ML projects**
- **Simple models like logistic regression can be surprisingly effective**
- **Model evaluation requires looking beyond overall accuracy**
- **Complex interactions between features can create predictive power even when individual features are weak**

### Key Findings Summary
1. **Excellent at predicting X wins** (74% precision, 99% recall)
2. **Outstanding at predicting O wins** (98% precision, 99% recall)  
3. **Complete failure on draws** (0% recall) - classic imbalanced data problem
4. **Strong overall performance** - 81.2% accuracy is impressive for this domain

### Next Steps
This analysis provides a solid foundation for:
1. Exploring more advanced ML algorithms
2. Understanding the importance of balanced evaluation
3. Learning about real-world ML challenges
4. Building more sophisticated prediction systems

**Bottom Line**: We successfully built a functional ML model that demonstrates both the power and limitations of machine learning on imbalanced datasets. The 81% accuracy is impressive and shows the model learned real strategic patterns, but the complete failure on draws teaches us valuable lessons about the importance of comprehensive model evaluation in real-world applications.
"""

# Tic-Tac-Toe Game using Trained ML Model
import time
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder

class TicTacToeML:
    def __init__(self, trained_model, label_encoders):
        """Initialize game with trained ML model and label encoders."""
        self.board = [' ' for _ in range(9)]  # Board positions 0-8
        self.player = 'O'  # Human player
        self.ai = 'X'      # AI player
        self.model = trained_model
        self.encoders = label_encoders
        self.move_history = []  # Track moves for ML prediction

    def print_board(self):
        """Display current board state."""
        print("\n   Current Board:")
        print("   1 | 2 | 3 ")
        print("  -----------")
        print("   4 | 5 | 6 ")
        print("  -----------")
        print("   7 | 8 | 9 ")
        print()

        print("   " + self.board[0] + " | " + self.board[1] + " | " + self.board[2])
        print("  -----------")
        print("   " + self.board[3] + " | " + self.board[4] + " | " + self.board[5])
        print("  -----------")
        print("   " + self.board[6] + " | " + self.board[7] + " | " + self.board[8])
        print()

    def position_to_rowcol(self, position):
        """Convert board position (0-8) to row-col format for ML model."""
        row = (position // 3) + 1
        col = (position % 3) + 1
        return f"{row}-{col}"

    def check_winner(self):
        """Check for winner or draw."""
        wins = [
            [0,1,2], [3,4,5], [6,7,8],  # Rows
            [0,3,6], [1,4,7], [2,5,8],  # Columns
            [0,4,8], [2,4,6]            # Diagonals
        ]

        for combo in wins:
            if (self.board[combo[0]] == self.board[combo[1]] == self.board[combo[2]] != ' '):
                return self.board[combo[0]]

        if ' ' not in self.board:
            return 'Draw'
        return None

    def get_empty_positions(self):
        """Return list of available positions."""
        return [i for i, spot in enumerate(self.board) if spot == ' ']

    def create_model_input(self, test_moves):
        """Create feature vector for ML model prediction."""
        features = {}

        # Define move columns matching training data format
        move_columns = [
            'Move 1-X (Row-Col)', 'Move 2-O (Row-Col)', 'Move 3-X (Row-Col)',
            'Move 4-O (Row-Col)', 'Move 5-X (Row-Col)', 'Move 6-O (Row-Col)',
            'Move 7-X (Row-Col)', 'Move 8-O (Row-Col)', 'Move 9-X (Row-Col)'
        ]

        # Fill moves alternating between X and O
        move_index = 0
        for i, move in enumerate(test_moves):
            if i % 2 == 0:  # X moves (first player)
                if move_index < len(move_columns) and 'X' in move_columns[move_index]:
                    features[move_columns[move_index]] = move
                    move_index += 1
            else:  # O moves (second player)
                if move_index < len(move_columns) and 'O' in move_columns[move_index]:
                    features[move_columns[move_index]] = move
                    move_index += 1

        # Fill remaining with placeholder
        for col in move_columns:
            if col not in features:
                features[col] = '---'

        features['Game_Length'] = len(test_moves)

        # Convert to DataFrame and apply label encoding
        df_input = pd.DataFrame([features])
        X_encoded = pd.DataFrame()

        for col in move_columns:
            if col in self.encoders:
                try:
                    encoded_values = []
                    for val in df_input[col]:
                        if val in self.encoders[col].classes_:
                            encoded_values.append(self.encoders[col].transform([val])[0])
                        else:
                            # Use placeholder encoding for unknown values
                            encoded_values.append(self.encoders[col].transform(['---'])[0])
                    X_encoded[col] = encoded_values
                except Exception:
                    X_encoded[col] = [0]

        X_encoded['Game_Length'] = df_input['Game_Length']
        return X_encoded

    def ai_move_with_model(self):
        """AI makes move using ML model predictions."""
        print("AI analyzing with ML model...")

        available_positions = self.get_empty_positions()

        # Check for immediate winning move
        for pos in available_positions:
            test_board = self.board.copy()
            test_board[pos] = self.ai
            if self.check_winner_on_board(test_board) == self.ai:
                self.board[pos] = self.ai
                self.move_history.append(self.position_to_rowcol(pos))
                print(f"AI wins with position {pos + 1}")
                return

        # Use ML model to evaluate positions
        best_position = None
        best_win_probability = -1
        model_predictions = {}

        print("ML model evaluating positions...")

        for pos in available_positions:
            test_moves = self.move_history.copy()
            test_moves.append(self.position_to_rowcol(pos))

            try:
                model_input = self.create_model_input(test_moves)
                probabilities = self.model.predict_proba(model_input)[0]

                # Extract AI win probability
                class_labels = self.model.classes_
                ai_prob = 0
                for i, label in enumerate(class_labels):
                    if label == self.ai:
                        ai_prob = probabilities[i]
                        break

                model_predictions[pos] = ai_prob
                print(f"   Position {pos+1}: {ai_prob:.3f} win probability")

                if ai_prob > best_win_probability:
                    best_win_probability = ai_prob
                    best_position = pos

            except Exception as e:
                print(f"   Position {pos+1}: Model error - {e}")
                model_predictions[pos] = 0

        # Make move based on model prediction
        if best_position is not None:
            self.board[best_position] = self.ai
            self.move_history.append(self.position_to_rowcol(best_position))
            print(f"AI chooses position {best_position + 1} (confidence: {best_win_probability:.3f})")

            # Show model rankings
            if len(model_predictions) > 1:
                sorted_preds = sorted(model_predictions.items(), key=lambda x: x[1], reverse=True)
                print(f"   Model ranking: {[f'Pos{p[0]+1}({p[1]:.3f})' for p in sorted_preds[:3]]}")
        else:
            # Fallback if model fails
            pos = available_positions[0]
            self.board[pos] = self.ai
            self.move_history.append(self.position_to_rowcol(pos))
            print(f"Model failed - using position {pos + 1}")

    def check_winner_on_board(self, test_board):
        """Check winner on specific board state."""
        wins = [
            [0,1,2], [3,4,5], [6,7,8],  # Rows
            [0,3,6], [1,4,7], [2,5,8],  # Columns
            [0,4,8], [2,4,6]            # Diagonals
        ]

        for combo in wins:
            if (test_board[combo[0]] == test_board[combo[1]] == test_board[combo[2]] != ' '):
                return test_board[combo[0]]

        if ' ' not in test_board:
            return 'Draw'
        return None

    def player_move(self):
        """Get human player move."""
        while True:
            try:
                move = input("Choose your position (1-9): ")
                position = int(move) - 1

                if position < 0 or position > 8:
                    print("Please enter a number between 1 and 9")
                    continue

                if self.board[position] != ' ':
                    print("That position is already taken")
                    continue

                self.board[position] = self.player
                self.move_history.append(self.position_to_rowcol(position))
                print(f"You chose position {move}")
                return position

            except ValueError:
                print("Please enter a valid number")

    def play_game(self):
        """Main game loop."""
        print("=" * 50)
        print("TIC-TAC-TOE vs TRAINED ML MODEL")
        print("=" * 50)
        print("You are O, AI is X")
        print("AI uses trained logistic regression model")
        print("Model trained on 255,168 games")
        print()
        print("Positions numbered 1-9:")
        print(" 1 | 2 | 3 ")
        print("-----------")
        print(" 4 | 5 | 6 ")
        print("-----------")
        print(" 7 | 8 | 9 ")
        print()

        self.move_history = []
        current_player = 'ai'  # AI goes first

        while True:
            self.print_board()

            if current_player == 'ai':
                self.ai_move_with_model()
                current_player = 'player'
            else:
                self.player_move()
                current_player = 'ai'

            winner = self.check_winner()
            if winner:
                self.print_board()
                if winner == self.ai:
                    print("AI WINS! Model predicted successfully.")
                elif winner == self.player:
                    print("YOU WIN! You beat the ML model.")
                else:
                    print("DRAW! No winner found.")

                print(f"\nGame moves: {' -> '.join(self.move_history)}")
                break

    def play_multiple_games(self):
        """Play multiple games with score tracking."""
        scores = {'player': 0, 'ai': 0, 'draws': 0}

        while True:
            self.board = [' ' for _ in range(9)]
            self.move_history = []
            self.play_game()

            winner = self.check_winner()
            if winner == self.ai:
                scores['ai'] += 1
            elif winner == self.player:
                scores['player'] += 1
            else:
                scores['draws'] += 1

            print(f"\nSCORE: You {scores['player']} - {scores['ai']} AI (Draws: {scores['draws']})")

            play_again = input("\nPlay again? (y/n): ").lower()
            if play_again != 'y':
                print("\nThanks for playing!")
                print(f"Final Score - You: {scores['player']}, AI: {scores['ai']}, Draws: {scores['draws']}")
                break

# Game initialization
print("STARTING TIC-TAC-TOE vs ML MODEL...")
print("Checking for trained model...")

try:
    if 'model' in globals() and ('le_dict' in globals() or 'df' in globals()):
        print("Found trained model and data")

        # Create label encoders
        move_columns = [col for col in df.columns if 'Move' in col and 'Row-Col' in col]
        encoders = {}
        for col in move_columns:
            le = LabelEncoder()
            le.fit(df[col].unique())
            encoders[col] = le

        # Mode selection
        print("\nCHOOSE GAME MODE:")
        print("1. PURE ML MODEL - AI uses only trained model")
        print("   (See what the 81% accuracy model learned)")
        print()
        print("2. HYBRID SMART AI - ML model + strategic safety")
        print("   (Better gameplay with model + win/block logic)")
        print()

        while True:
            try:
                choice = input("Choose mode (1 or 2): ").strip()
                if choice in ['1', '2']:
                    break
                print("Please enter 1 or 2")
            except:
                print("Please enter 1 or 2")

        if choice == '1':
            print("\nStarting PURE ML MODEL mode...")
            print("AI uses only trained model (minimal overrides)")
            game = TicTacToeML(model, encoders)
            game.play_multiple_games()

        else:  # choice == '2'
            print("\nStarting HYBRID SMART AI mode...")
            print("AI uses ML model + strategic safety checks")
            print("Hybrid mode not implemented yet!")
            print("Using Pure ML mode instead...")
            game = TicTacToeML(model, encoders)
            game.play_multiple_games()

    else:
        print("Model or data not found!")
        print("Run ML training code first")
        print("Make sure 'model' and 'df' variables exist")

except Exception as e:
    print(f"Error starting game: {e}")
    print("Run ML training code first!")

    # Debug info
    print("\nDEBUG INFO:")
    print(f"'model' exists: {'model' in globals()}")
    print(f"'df' exists: {'df' in globals()}")
    print(f"'le_dict' exists: {'le_dict' in globals()}")

# Simple correlation matrix with what we have
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Create simple features for correlation
corr_data = pd.DataFrame()

# Add outcomes as numbers
corr_data['X_Wins'] = (df['Winner'] == 'X').astype(int)
corr_data['O_Wins'] = (df['Winner'] == 'O').astype(int)
corr_data['Draws'] = (df['Winner'] == '-').astype(int)

# Add game length
corr_data['Game_Length'] = df['Game_Length']

# Add first move type (if the column exists)
if 'First_Move_Name' in df.columns:
    corr_data['Center_First'] = (df['First_Move_Name'] == 'CENTER').astype(int)
    corr_data['Corner_First'] = df['First_Move_Name'].isin(['Top-Left', 'Top-Right', 'Bottom-Left', 'Bottom-Right']).astype(int)
    corr_data['Edge_First'] = df['First_Move_Name'].isin(['Top-Center', 'Middle-Left', 'Middle-Right', 'Bottom-Center']).astype(int)

# Calculate correlation
corr_matrix = corr_data.corr()

# Plot it
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix,
            annot=True,
            fmt='.2f',
            cmap='coolwarm',
            center=0,
            square=True,
            linewidths=1)
plt.title('Correlation Matrix')
plt.show()

"""# Machine Learning Model Performance Analysis

##  What We Observed Playing Against Our Model

### Current Model Behavior
- **✅ Good**: Achieves 81% accuracy on test data
- **✅ Good**: Makes reasonable opening moves
- **❌ Poor**: Doesn't block obvious winning moves
- **❌ Poor**: Win probabilities are very similar (all around 5-6%)
- **❌ Poor**: Doesn't show clear strategic preference differences

### Key Limitation: **Our Model Predicts Final Game Outcomes, Not Optimal Next Moves**

---

##  Root Cause Analysis

### What Our Model Actually Learned
Our logistic regression model was trained to answer: *"Given this complete game sequence, who won?"*

**Training Data Format:**
```
Move 1-X: 2-2, Move 2-O: 1-1, Move 3-X: 3-3, ... → Winner: X
```

### What We're Asking It To Do
We're asking: *"Given this partial game, what's the best next move?"*

**Game Usage:**
```
Current: Move 1-X: 2-2, Move 2-O: 1-1
Question: Where should X move next to maximize win probability?
```

### The Mismatch Problem
1. **Training**: Model learned patterns from complete 5-9 move games
2. **Usage**: We're asking it to evaluate 1-3 move partial games
3. **Result**: Model sees incomplete patterns and gives uncertain predictions

---

## Why Win Probabilities Are So Similar

### Statistical Reality
- In early game positions, most moves lead to similar win rates
- Strategic advantage often becomes clear only in mid-to-late game
- Our model learned "if game reaches 7+ moves, these patterns predict winner"
- But we're asking "after 2 moves, which next move is best?"

### Example Analysis
When you take center (2-2), the model sees:
```
Move 1-O: 2-2, Move 2-X: ???
```
It doesn't have enough context to strongly differentiate between AI response options because most 2-move games in training data had similar outcomes regardless of the second move.



##  Immediate Improvements for Current Model

### 1. Feature Engineering
Add strategic features to training data:
- **Center control**: Is center occupied?
- **Corner control**: How many corners controlled?
- **Threat count**: Number of two-in-a-row situations
- **Blocking needs**: Opponent threats to block

### 2. Different Target Variables
Instead of just predicting winner, predict:
- **Move quality scores**: Rate each move 1-10
- **Position advantage**: Numerical board evaluation
- **Threat level**: How dangerous is current position

### 3. Ensemble Methods
Combine multiple approaches:
```python
final_decision = weighted_average([
    ml_model_prediction,
    minimax_search_result,
    strategic_heuristics
])
```

### 4. Better Training Data
- **Add board states**: Not just move sequences
- **Include move annotations**: Mark good/bad moves
- **Generate synthetic data**: Create perfect play examples


##  Key Insights

### What We Learned
1. **High accuracy ≠ Good gameplay**: 81% accuracy doesn't mean strategic play
2. **Training objective matters**: Predicting outcomes ≠ choosing moves
3. **Context is crucial**: Early game vs late game require different strategies
4. **Simple models have limits**: Logistic regression can't capture complex strategic patterns

### What This Teaches About ML
1. **Define the right problem**: Train for what you actually need
2. **Feature engineering is key**: Board state > move sequences for strategy
3. **Domain knowledge helps**: Pure ML often needs strategic guidance
4. **Evaluation matters**: Test on actual use case, not just accuracy metrics

---

##  Conclusion

Our 81% accurate model demonstrates both the **power and limitations** of machine learning:

**✅ Successes:**
- Learned real patterns from 255k games
- Achieved impressive statistical accuracy
- Made reasonable strategic decisions

**❌ Limitations:**
- Wrong training objective (outcome vs move quality)
- Missing strategic features (board state, threats)
- Can't handle early game uncertainty well

** The Path Forward:**
The solution isn't to abandon ML, but to **design it better**:
- Train on the right objective (move quality)
- Add strategic features (board evaluation)
- Combine ML with traditional game AI techniques

"""

# Random Forest Implementation with Full Analysis
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_recall_fscore_support
from sklearn.preprocessing import LabelEncoder
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time

print("="*70)
print("RANDOM FOREST CLASSIFIER - COMPLETE ANALYSIS")
print("="*70)

# Using your existing data preparation from the notebook
print("\n📊 PREPARING DATA...")

# Get move columns (same as in your notebook)
move_columns = [col for col in df.columns if 'Move' in col and 'Row-Col' in col]
print(f"Found {len(move_columns)} move columns to use as features")

# Encode the features (same as your notebook)
print("\nConverting text moves to numbers...")
le_dict = {}  # Store our encoders
X_encoded = pd.DataFrame()

for col in move_columns:
    le = LabelEncoder()
    unique_values = df[col].unique()
    le.fit(unique_values)
    le_dict[col] = le
    X_encoded[col] = le.transform(df[col])

# Add game length feature
X_encoded['Game_Length'] = df['Game_Length']

# Target variable
y = df['Winner']

print(f"\nTotal dataset size: {len(df):,} games")
print(f"Number of features: {X_encoded.shape[1]}")

# Train-Test Split (70-30) with same random state as your notebook
X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(
    X_encoded, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\n✅ Data Split Complete:")
print(f"Training set: {len(X_train_rf):,} games (70%)")
print(f"Testing set: {len(X_test_rf):,} games (30%)")

# Check class distribution
print("\n📈 Class Distribution in Training Set:")
for outcome, count in y_train_rf.value_counts().items():
    percentage = (count / len(y_train_rf)) * 100
    print(f"  {outcome}: {count:,} games ({percentage:.1f}%)")

# Train Random Forest Model
print("\n🌲 TRAINING RANDOM FOREST MODEL...")
rf_model = RandomForestClassifier(
    n_estimators=200,           # Number of trees
    max_depth=15,               # Maximum depth of trees
    min_samples_split=10,       # Minimum samples to split
    min_samples_leaf=5,         # Minimum samples in leaf
    max_features='sqrt',        # Features to consider at each split
    class_weight='balanced',    # Handle class imbalance
    random_state=42,
    n_jobs=-1,                  # Use all CPU cores
    verbose=1
)

# Train the model
start_time = time.time()
rf_model.fit(X_train_rf, y_train_rf)
training_time = time.time() - start_time

print(f"\n✅ Model trained in {training_time:.2f} seconds")
print(f"Number of trees: {rf_model.n_estimators}")

# Make Predictions
print("\n🎯 MAKING PREDICTIONS...")
rf_pred = rf_model.predict(X_test_rf)
rf_pred_proba = rf_model.predict_proba(X_test_rf)

# Calculate Metrics
rf_accuracy = accuracy_score(y_test_rf, rf_pred)
rf_f1 = f1_score(y_test_rf, rf_pred, average='weighted')
precision, recall, f1, support = precision_recall_fscore_support(y_test_rf, rf_pred, average=None, labels=['X', 'O', '-'])

print(f"\n📊 RANDOM FOREST PERFORMANCE:")
print(f"Overall Accuracy: {rf_accuracy:.3f} ({rf_accuracy*100:.1f}%)")
print(f"Weighted F1 Score: {rf_f1:.3f}")

# Comparison with your Logistic Regression model
# Using the accuracy variable from your notebook
if 'accuracy' in globals():
    improvement = ((rf_accuracy - accuracy) / accuracy) * 100
    print(f"\n📈 Improvement over Logistic Regression: {improvement:+.1f}%")

# Feature Importance Analysis
feature_importance = pd.DataFrame({
    'feature': X_train_rf.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n🔍 TOP 10 MOST IMPORTANT FEATURES:")
for idx, row in feature_importance.head(10).iterrows():
    print(f"{row['feature']:20s}: {row['importance']:.4f}")

# Create Comprehensive Visualizations
fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)

# 1. Model Comparison Bar Chart
ax1 = fig.add_subplot(gs[0, :2])

# Check if logistic regression results exist from your notebook
if 'accuracy' in globals() and 'y_pred' in globals():
    models_data = {
        'Logistic\nRegression': [accuracy * 100, f1_score(y_test, y_pred, average='weighted') * 100],
        'Random\nForest': [rf_accuracy * 100, rf_f1 * 100]
    }
else:
    # If not, just show Random Forest
    models_data = {
        'Random\nForest': [rf_accuracy * 100, rf_f1 * 100]
    }

x = np.arange(len(models_data))
width = 0.35
bars1 = ax1.bar(x - width/2, [models_data[m][0] for m in models_data], width,
                label='Accuracy', color='lightcoral', alpha=0.8)
bars2 = ax1.bar(x + width/2, [models_data[m][1] for m in models_data], width,
                label='F1 Score', color='lightblue', alpha=0.8)

ax1.set_xlabel('Model')
ax1.set_ylabel('Score (%)')
ax1.set_title('Model Performance Comparison', fontsize=16, fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels(models_data.keys())
ax1.legend()
ax1.grid(axis='y', alpha=0.3)

# Add value labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                 f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')

# 2. Feature Importance Plot
ax2 = fig.add_subplot(gs[0, 2])
top_features = feature_importance.head(10)
bars = ax2.barh(range(len(top_features)), top_features['importance'], color='forestgreen', alpha=0.8)
ax2.set_yticks(range(len(top_features)))
ax2.set_yticklabels(top_features['feature'], fontsize=9)
ax2.set_xlabel('Importance Score')
ax2.set_title('Top 10 Feature Importances', fontsize=14, fontweight='bold')
ax2.grid(axis='x', alpha=0.3)

# 3. Confusion Matrix
ax3 = fig.add_subplot(gs[1, 0])
cm_rf = confusion_matrix(y_test_rf, rf_pred, labels=['X', 'O', '-'])
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=ax3,
            xticklabels=['X', 'O', 'Draw'], yticklabels=['X', 'O', 'Draw'],
            cbar_kws={'label': 'Count'})
ax3.set_title('Random Forest Confusion Matrix', fontsize=14, fontweight='bold')
ax3.set_xlabel('Predicted')
ax3.set_ylabel('Actual')

# 4. Class-wise Performance
ax4 = fig.add_subplot(gs[1, 1])
class_metrics = pd.DataFrame({
    'Precision': precision,
    'Recall': recall,
    'F1-Score': f1
}, index=['X Wins', 'O Wins', 'Draws'])

class_metrics.plot(kind='bar', ax=ax4, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)
ax4.set_title('Performance by Outcome Class', fontsize=14, fontweight='bold')
ax4.set_ylabel('Score')
ax4.set_xlabel('Outcome')
ax4.legend(loc='lower right')
ax4.grid(axis='y', alpha=0.3)
ax4.set_xticklabels(['X Wins', 'O Wins', 'Draws'], rotation=0)

# 5. Prediction Confidence Distribution
ax5 = fig.add_subplot(gs[1, 2])
rf_max_proba = np.max(rf_pred_proba, axis=1)
ax5.hist(rf_max_proba, bins=50, color='forestgreen', alpha=0.8, edgecolor='black')
ax5.set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')
ax5.set_xlabel('Maximum Probability')
ax5.set_ylabel('Frequency')
ax5.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='50% threshold')
ax5.axvline(x=rf_max_proba.mean(), color='orange', linestyle='--', alpha=0.7,
            label=f'Mean: {rf_max_proba.mean():.2f}')
ax5.legend()
ax5.grid(axis='y', alpha=0.3)

# 6. Prediction Distribution Comparison
ax6 = fig.add_subplot(gs[2, :2])
pred_comparison = pd.DataFrame({
    'Actual': y_test_rf.value_counts().reindex(['X', 'O', '-'], fill_value=0),
    'Random Forest': pd.Series(rf_pred).value_counts().reindex(['X', 'O', '-'], fill_value=0)
})

# Add logistic regression if available
if 'y_pred' in globals():
    pred_comparison['Logistic Regression'] = pd.Series(y_pred).value_counts().reindex(['X', 'O', '-'], fill_value=0)

pred_comparison.plot(kind='bar', ax=ax6, alpha=0.8)
ax6.set_title('Prediction Distribution: Actual vs Models', fontsize=14, fontweight='bold')
ax6.set_xlabel('Outcome')
ax6.set_ylabel('Number of Games')
ax6.legend(title='Distribution')
ax6.grid(axis='y', alpha=0.3)
ax6.set_xticklabels(['X Wins', 'O Wins', 'Draws'], rotation=0)

# 7. Error Analysis
ax7 = fig.add_subplot(gs[2, 2])
errors = y_test_rf != rf_pred
error_types = []
for i in range(len(y_test_rf)):
    if errors.iloc[i]:
        error_types.append(f"{y_test_rf.iloc[i]}→{rf_pred[i]}")

error_counts = pd.Series(error_types).value_counts().head(6)
ax7.bar(range(len(error_counts)), error_counts.values, color='salmon', alpha=0.8)
ax7.set_title('Most Common Prediction Errors', fontsize=14, fontweight='bold')
ax7.set_xlabel('Error Type (Actual→Predicted)')
ax7.set_ylabel('Count')
ax7.set_xticks(range(len(error_counts)))
ax7.set_xticklabels(error_counts.index, rotation=45, ha='right')
ax7.grid(axis='y', alpha=0.3)

# 8. Feature Importance by Move Number
ax8 = fig.add_subplot(gs[3, 0])
# Group features by move number
early_moves = feature_importance[feature_importance['feature'].str.contains('Move [1-3]')]['importance'].sum()
mid_moves = feature_importance[feature_importance['feature'].str.contains('Move [4-6]')]['importance'].sum()
late_moves = feature_importance[feature_importance['feature'].str.contains('Move [7-9]')]['importance'].sum()
game_length_imp = feature_importance[feature_importance['feature'] == 'Game_Length']['importance'].sum()

categories = ['Early Moves\n(1-3)', 'Mid Moves\n(4-6)', 'Late Moves\n(7-9)', 'Game Length']
importances = [early_moves, mid_moves, late_moves, game_length_imp]

bars = ax8.bar(categories, importances, color=['lightblue', 'lightgreen', 'lightyellow', 'lightcoral'],
               alpha=0.8, edgecolor='black')
ax8.set_title('Feature Importance by Game Phase', fontsize=14, fontweight='bold')
ax8.set_ylabel('Cumulative Importance')
ax8.grid(axis='y', alpha=0.3)

for bar, imp in zip(bars, importances):
    ax8.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.001,
             f'{imp:.3f}', ha='center', va='bottom', fontweight='bold')

# 9. Draw Prediction Analysis
ax9 = fig.add_subplot(gs[3, 1:])
draw_analysis = {
    'Random Forest': {
        'Predicted': np.sum(rf_pred == '-'),
        'Actual': np.sum(y_test_rf == '-')
    }
}

# Add logistic regression comparison if available
if 'y_pred' in globals() and 'y_test' in globals():
    draw_analysis['Logistic Regression'] = {
        'Predicted': np.sum(y_pred == '-'),
        'Actual': np.sum(y_test == '-')
    }

models = list(draw_analysis.keys())
predicted_draws = [draw_analysis[m]['Predicted'] for m in models]
actual_draws = [draw_analysis[m]['Actual'] for m in models]

x = np.arange(len(models))
width = 0.35
bars1 = ax9.bar(x - width/2, predicted_draws, width, label='Predicted Draws', color='lightcoral', alpha=0.8)
bars2 = ax9.bar(x + width/2, actual_draws, width, label='Actual Draws', color='lightblue', alpha=0.8)

ax9.set_xlabel('Model')
ax9.set_ylabel('Number of Draws')
ax9.set_title('Draw Prediction Capability', fontsize=14, fontweight='bold')
ax9.set_xticks(x)
ax9.set_xticklabels(models)
ax9.legend()
ax9.grid(axis='y', alpha=0.3)

# Add value labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax9.text(bar.get_x() + bar.get_width()/2., height + 50,
                 f'{int(height):,}', ha='center', va='bottom', fontweight='bold')

plt.suptitle('Random Forest Model - Comprehensive Analysis', fontsize=18, fontweight='bold', y=0.995)
plt.tight_layout()
plt.show()

# Detailed Performance Report
print("\n📋 DETAILED CLASSIFICATION REPORT:")
print("="*60)
print(classification_report(y_test_rf, rf_pred, target_names=['Draw', 'O Wins', 'X Wins']))

# Key Insights
print("\n💡 KEY INSIGHTS FROM RANDOM FOREST MODEL:")
print("="*50)

# Calculate draw prediction improvement
rf_draw_predictions = np.sum(rf_pred == '-')
actual_draws_test = np.sum(y_test_rf == '-')

print(f"\n1. DRAW PREDICTION IMPROVEMENT:")
if 'y_pred' in globals():
    lr_draw_predictions = np.sum(y_pred == '-')
    print(f"   • Logistic Regression predicted: {lr_draw_predictions} draws")
    print(f"   • Random Forest predicted: {rf_draw_predictions} draws")
else:
    print(f"   • Random Forest predicted: {rf_draw_predictions} draws")
print(f"   • Actual draws in test set: {actual_draws_test}")

if rf_draw_predictions > 0:
    draw_recall = recall[2]  # Draw is the third class
    print(f"   • Draw recall rate: {draw_recall:.1%}")

print(f"\n2. OVERALL PERFORMANCE:")
print(f"   • Random Forest Accuracy: {rf_accuracy*100:.1f}%")
if 'accuracy' in globals():
    print(f"   • Logistic Regression Accuracy: {accuracy*100:.1f}%")
    print(f"   • Improvement: {improvement:+.1f}%")

print(f"\n3. MOST IMPORTANT FEATURES:")
for i, (idx, row) in enumerate(feature_importance.head(5).iterrows()):
    print(f"   {i+1}. {row['feature']}: {row['importance']:.3f}")

print(f"\n4. PREDICTION CONFIDENCE:")
print(f"   • Average confidence: {rf_max_proba.mean():.3f}")
print(f"   • High confidence (>90%): {np.sum(rf_max_proba > 0.9)} predictions")
print(f"   • Low confidence (<60%): {np.sum(rf_max_proba < 0.6)} predictions")

print(f"\n5. ERROR ANALYSIS:")
total_errors = np.sum(errors)
error_rate = (total_errors / len(y_test_rf)) * 100
print(f"   • Total misclassifications: {total_errors:,} ({error_rate:.1f}%)")
if len(error_counts) > 0:
    print(f"   • Most common error: {error_counts.index[0]} ({error_counts.iloc[0]} times)")

print("\n🎯 SUMMARY:")
print("The Random Forest model shows significant improvements,")
print("particularly in handling the class imbalance problem. The ensemble nature of")
print("Random Forest allows it to capture more complex patterns in the game data.")

# Save the model and encoders for later use
print("\n💾 Saving model and encoders...")
rf_model_data = {
    'model': rf_model,
    'encoders': le_dict,
    'feature_columns': move_columns,
    'accuracy': rf_accuracy
}
print("Model saved to rf_model_data dictionary")

# Neural Network Implementation with Full Analysis
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import time
warnings.filterwarnings('ignore')

print("="*70)
print("NEURAL NETWORK (MULTI-LAYER PERCEPTRON) - COMPLETE ANALYSIS")
print("="*70)

# Using your existing data preparation
print("\n📊 PREPARING DATA FOR NEURAL NETWORK...")

# Get move columns (same as in your notebook)
move_columns = [col for col in df.columns if 'Move' in col and 'Row-Col' in col]
print(f"Found {len(move_columns)} move columns to use as features")

# Encode the features (same as your notebook)
print("\nConverting text moves to numbers...")
le_dict = {}  # Store our encoders
X_encoded = pd.DataFrame()

for col in move_columns:
    le = LabelEncoder()
    unique_values = df[col].unique()
    le.fit(unique_values)
    le_dict[col] = le
    X_encoded[col] = le.transform(df[col])

# Add game length feature
X_encoded['Game_Length'] = df['Game_Length']

# Target variable
y = df['Winner']

print(f"\nTotal dataset size: {len(df):,} games")
print(f"Number of features: {X_encoded.shape[1]}")

# Train-Test Split (70-30)
X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(
    X_encoded, y, test_size=0.3, random_state=42, stratify=y
)

# Label encode the target variable for neural network
print("\n🔧 ENCODING TARGET VARIABLE...")
label_encoder_y = LabelEncoder()
y_train_nn_encoded = label_encoder_y.fit_transform(y_train_nn)
y_test_nn_encoded = label_encoder_y.transform(y_test_nn)

print("Target encoding:")
for i, label in enumerate(label_encoder_y.classes_):
    print(f"   {label} → {i}")

print(f"\n✅ Data Split Complete:")
print(f"Training set: {len(X_train_nn):,} games (70%)")
print(f"Testing set: {len(X_test_nn):,} games (30%)")

# Feature Scaling (Important for Neural Networks)
print("\n🔧 SCALING FEATURES...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_nn)
X_test_scaled = scaler.transform(X_test_nn)
print("✅ Features scaled to standard normal distribution")

# Define Neural Network Architecture
print("\n🧠 NEURAL NETWORK ARCHITECTURE:")
print("Input Layer: {} neurons (features)".format(X_train_scaled.shape[1]))
print("Hidden Layer 1: 128 neurons (ReLU activation)")
print("Hidden Layer 2: 64 neurons (ReLU activation)")
print("Hidden Layer 3: 32 neurons (ReLU activation)")
print("Output Layer: 3 neurons (softmax activation)")

# Train Neural Network
print("\n🚀 TRAINING NEURAL NETWORK...")
mlp_model = MLPClassifier(
    hidden_layer_sizes=(128, 64, 32),  # 3 hidden layers
    activation='relu',                   # ReLU activation
    solver='adam',                       # Adam optimizer
    alpha=0.001,                        # L2 regularization
    batch_size='auto',                  # Let sklearn decide
    learning_rate='constant',
    learning_rate_init=0.001,
    max_iter=500,                       # Maximum iterations
    shuffle=True,
    random_state=42,
    early_stopping=True,                # Stop if validation score stops improving
    validation_fraction=0.15,           # 15% of training data for validation
    n_iter_no_change=20,               # Patience for early stopping
    verbose=True
)

# Train the model
start_time = time.time()
mlp_model.fit(X_train_scaled, y_train_nn_encoded)
training_time = time.time() - start_time

print(f"\n✅ Model trained in {training_time:.2f} seconds")
print(f"Training stopped at iteration: {mlp_model.n_iter_}")
print(f"Final training loss: {mlp_model.loss_:.4f}")

# Make Predictions
print("\n🎯 MAKING PREDICTIONS...")
nn_pred_encoded = mlp_model.predict(X_test_scaled)
nn_pred = label_encoder_y.inverse_transform(nn_pred_encoded)
nn_pred_proba = mlp_model.predict_proba(X_test_scaled)

# Calculate Metrics
nn_accuracy = accuracy_score(y_test_nn, nn_pred)
nn_f1 = f1_score(y_test_nn, nn_pred, average='weighted')

print(f"\n📊 NEURAL NETWORK PERFORMANCE:")
print(f"Overall Accuracy: {nn_accuracy:.3f} ({nn_accuracy*100:.1f}%)")
print(f"Weighted F1 Score: {nn_f1:.3f}")

# Comparison with other models
if 'accuracy' in globals():
    lr_improvement = ((nn_accuracy - accuracy) / accuracy) * 100
    print(f"\n📈 Improvement over Logistic Regression: {lr_improvement:+.1f}%")

if 'rf_accuracy' in globals():
    rf_improvement = ((nn_accuracy - rf_accuracy) / rf_accuracy) * 100
    print(f"📈 Compared to Random Forest: {rf_improvement:+.1f}%")

# Create Comprehensive Visualizations
fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)

# 1. Training History
ax1 = fig.add_subplot(gs[0, :2])
ax1.plot(mlp_model.loss_curve_, 'b-', linewidth=2, label='Training Loss')
if hasattr(mlp_model, 'validation_scores_'):
    ax1_twin = ax1.twinx()
    ax1_twin.plot(mlp_model.validation_scores_, 'r-', linewidth=2, label='Validation Score')
    ax1_twin.set_ylabel('Validation Score', color='r')
    ax1_twin.tick_params(axis='y', labelcolor='r')
ax1.set_xlabel('Iteration')
ax1.set_ylabel('Training Loss', color='b')
ax1.tick_params(axis='y', labelcolor='b')
ax1.set_title('Neural Network Training History', fontsize=16, fontweight='bold')
ax1.grid(True, alpha=0.3)
ax1.legend(loc='upper left')

# 2. Model Architecture Visualization
ax2 = fig.add_subplot(gs[0, 2])
ax2.text(0.5, 0.9, 'Neural Network Architecture', ha='center', fontsize=14, fontweight='bold')
ax2.text(0.5, 0.75, f'Input: {X_train_scaled.shape[1]} features', ha='center', fontsize=10)
ax2.text(0.5, 0.60, 'Hidden 1: 128 neurons (ReLU)', ha='center', fontsize=10)
ax2.text(0.5, 0.45, 'Hidden 2: 64 neurons (ReLU)', ha='center', fontsize=10)
ax2.text(0.5, 0.30, 'Hidden 3: 32 neurons (ReLU)', ha='center', fontsize=10)
ax2.text(0.5, 0.15, 'Output: 3 classes (Softmax)', ha='center', fontsize=10)
ax2.set_xlim(0, 1)
ax2.set_ylim(0, 1)
ax2.axis('off')

# Draw connections
for y in [0.70, 0.55, 0.40, 0.25]:
    ax2.plot([0.3, 0.7], [y, y-0.05], 'k-', alpha=0.3)
    ax2.plot([0.3, 0.7], [y, y+0.05], 'k-', alpha=0.3)

# 3. Confusion Matrix
ax3 = fig.add_subplot(gs[1, 0])
cm_nn = confusion_matrix(y_test_nn, nn_pred, labels=['X', 'O', '-'])
sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Blues', ax=ax3,
            xticklabels=['X', 'O', 'Draw'], yticklabels=['X', 'O', 'Draw'],
            cbar_kws={'label': 'Count'})
ax3.set_title('Neural Network Confusion Matrix', fontsize=14, fontweight='bold')
ax3.set_xlabel('Predicted')
ax3.set_ylabel('Actual')

# 4. Model Comparison
ax4 = fig.add_subplot(gs[1, 1])
models_comparison = {}

# Add models that exist
if 'accuracy' in globals():
    models_comparison['Logistic\nRegression'] = accuracy * 100
if 'rf_accuracy' in globals():
    models_comparison['Random\nForest'] = rf_accuracy * 100
models_comparison['Neural\nNetwork'] = nn_accuracy * 100

colors = ['lightcoral', 'forestgreen', 'royalblue'][:len(models_comparison)]
bars = ax4.bar(models_comparison.keys(), models_comparison.values(), color=colors, alpha=0.8)
ax4.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')
ax4.set_ylabel('Accuracy (%)')
ax4.set_ylim(0, 100)
ax4.grid(axis='y', alpha=0.3)

for bar in bars:
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2., height + 1,
             f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')

# 5. Prediction Confidence Distribution
ax5 = fig.add_subplot(gs[1, 2])
nn_max_proba = np.max(nn_pred_proba, axis=1)
ax5.hist(nn_max_proba, bins=50, color='royalblue', alpha=0.8, edgecolor='black')
ax5.set_title('Neural Network Prediction Confidence', fontsize=14, fontweight='bold')
ax5.set_xlabel('Maximum Probability')
ax5.set_ylabel('Frequency')
ax5.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='50% threshold')
ax5.axvline(x=nn_max_proba.mean(), color='orange', linestyle='--', alpha=0.7,
            label=f'Mean: {nn_max_proba.mean():.2f}')
ax5.legend()
ax5.grid(axis='y', alpha=0.3)

# 6. Class-wise Performance
ax6 = fig.add_subplot(gs[2, 0])
# Calculate per-class metrics
precision = []
recall = []
f1_scores = []
for cls in ['X', 'O', '-']:
    mask = y_test_nn == cls
    if mask.sum() > 0:
        prec = np.sum((y_test_nn == cls) & (nn_pred == cls)) / np.sum(nn_pred == cls) if np.sum(nn_pred == cls) > 0 else 0
        rec = np.sum((y_test_nn == cls) & (nn_pred == cls)) / np.sum(y_test_nn == cls)
        f1 = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0
        precision.append(prec)
        recall.append(rec)
        f1_scores.append(f1)

x = np.arange(3)
width = 0.25
bars1 = ax6.bar(x - width, precision, width, label='Precision', color='#FF6B6B', alpha=0.8)
bars2 = ax6.bar(x, recall, width, label='Recall', color='#4ECDC4', alpha=0.8)
bars3 = ax6.bar(x + width, f1_scores, width, label='F1-Score', color='#45B7D1', alpha=0.8)

ax6.set_xlabel('Outcome Class')
ax6.set_ylabel('Score')
ax6.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')
ax6.set_xticks(x)
ax6.set_xticklabels(['X Wins', 'O Wins', 'Draws'])
ax6.legend()
ax6.grid(axis='y', alpha=0.3)

# 7. Prediction Error Analysis
ax7 = fig.add_subplot(gs[2, 1])
errors = y_test_nn != nn_pred
error_indices = np.where(errors)[0]
error_types = []
for idx in error_indices:
    error_types.append(f"{y_test_nn.iloc[idx]}→{nn_pred[idx]}")

error_counts = pd.Series(error_types).value_counts().head(6)
if len(error_counts) > 0:
    ax7.bar(range(len(error_counts)), error_counts.values, color='salmon', alpha=0.8)
    ax7.set_title('Most Common Prediction Errors', fontsize=14, fontweight='bold')
    ax7.set_xlabel('Error Type (Actual→Predicted)')
    ax7.set_ylabel('Count')
    ax7.set_xticks(range(len(error_counts)))
    ax7.set_xticklabels(error_counts.index, rotation=45, ha='right')
    ax7.grid(axis='y', alpha=0.3)

# 8. Network Statistics
ax8 = fig.add_subplot(gs[2, 2])
total_params = (X_train_scaled.shape[1] * 128) + (128 * 64) + (64 * 32) + (32 * 3) + 128 + 64 + 32 + 3
ax8.text(0.5, 0.9, 'Neural Network Statistics', ha='center', fontsize=14, fontweight='bold')
ax8.text(0.5, 0.7, f'Total Parameters: {total_params:,}', ha='center')
ax8.text(0.5, 0.5, f'Training Iterations: {mlp_model.n_iter_}', ha='center')
ax8.text(0.5, 0.3, f'Final Loss: {mlp_model.loss_:.4f}', ha='center')
ax8.text(0.5, 0.1, f'Early Stopping: {"Yes" if mlp_model.n_iter_ < 500 else "No"}', ha='center')
ax8.set_xlim(0, 1)
ax8.set_ylim(0, 1)
ax8.axis('off')

# 9. Confidence-based Accuracy Analysis
ax9 = fig.add_subplot(gs[3, :])
# Compare predictions at different confidence levels
confidence_bins = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
bin_accuracies = []
bin_counts = []

for i in range(len(confidence_bins)-1):
    mask = (nn_max_proba >= confidence_bins[i]) & (nn_max_proba < confidence_bins[i+1])
    if mask.sum() > 0:
        bin_accuracy = (y_test_nn[mask] == nn_pred[mask]).mean()
        bin_accuracies.append(bin_accuracy)
        bin_counts.append(mask.sum())
    else:
        bin_accuracies.append(0)
        bin_counts.append(0)

x_pos = np.arange(len(bin_accuracies))
bars = ax9.bar(x_pos, bin_accuracies, color='royalblue', alpha=0.8)
ax9.set_xlabel('Confidence Range')
ax9.set_ylabel('Accuracy')
ax9.set_title('Accuracy vs Prediction Confidence', fontsize=14, fontweight='bold')
ax9.set_xticks(x_pos)
ax9.set_xticklabels([f'{confidence_bins[i]:.1f}-{confidence_bins[i+1]:.1f}'
                      for i in range(len(confidence_bins)-1)], rotation=45)
ax9.grid(axis='y', alpha=0.3)

# Add count labels
for i, (acc, count) in enumerate(zip(bin_accuracies, bin_counts)):
    if count > 0:
        ax9.text(i, acc + 0.01, f'{count}', ha='center', va='bottom', fontsize=8)

plt.suptitle('Neural Network Model - Comprehensive Analysis', fontsize=18, fontweight='bold', y=0.995)
plt.tight_layout()
plt.show()

# Detailed Performance Report
print("\n📋 DETAILED CLASSIFICATION REPORT:")
print("="*60)
print(classification_report(y_test_nn, nn_pred, target_names=['Draw', 'O Wins', 'X Wins']))

# Additional Neural Network Insights
print("\n🧠 NEURAL NETWORK SPECIFIC INSIGHTS:")
print("="*50)

# Analyze confidence levels
high_confidence = nn_max_proba > 0.9
medium_confidence = (nn_max_proba > 0.6) & (nn_max_proba <= 0.9)
low_confidence = nn_max_proba <= 0.6

print(f"\n1. PREDICTION CONFIDENCE ANALYSIS:")
print(f"   • High confidence (>90%): {high_confidence.sum()} ({high_confidence.mean()*100:.1f}%)")
print(f"   • Medium confidence (60-90%): {medium_confidence.sum()} ({medium_confidence.mean()*100:.1f}%)")
print(f"   • Low confidence (≤60%): {low_confidence.sum()} ({low_confidence.mean()*100:.1f}%)")

# Accuracy by confidence
print(f"\n2. ACCURACY BY CONFIDENCE LEVEL:")
if high_confidence.sum() > 0:
    print(f"   • High confidence accuracy: {(y_test_nn[high_confidence] == nn_pred[high_confidence]).mean()*100:.1f}%")
if medium_confidence.sum() > 0:
    print(f"   • Medium confidence accuracy: {(y_test_nn[medium_confidence] == nn_pred[medium_confidence]).mean()*100:.1f}%")
if low_confidence.sum() > 0:
    print(f"   • Low confidence accuracy: {(y_test_nn[low_confidence] == nn_pred[low_confidence]).mean()*100:.1f}%")

# Draw predictions
nn_draw_predictions = np.sum(nn_pred == '-')
actual_draws = np.sum(y_test_nn == '-')

print(f"\n3. DRAW PREDICTION CAPABILITY:")
print(f"   • Predicted draws: {nn_draw_predictions}")
print(f"   • Actual draws: {actual_draws}")
if nn_draw_predictions > 0:
    draw_precision = np.sum((y_test_nn == '-') & (nn_pred == '-')) / nn_draw_predictions
    draw_recall = np.sum((y_test_nn == '-') & (nn_pred == '-')) / actual_draws
    print(f"   • Draw precision: {draw_precision:.1%}")
    print(f"   • Draw recall: {draw_recall:.1%}")

print(f"\n4. NETWORK COMPLEXITY:")
print(f"   • Total trainable parameters: {total_params:,}")
print(f"   • Parameters per training example: {total_params/len(X_train_nn):.2f}")

print(f"\n5. TRAINING EFFICIENCY:")
print(f"   • Training time: {training_time:.2f} seconds")
print(f"   • Time per epoch: {training_time/mlp_model.n_iter_:.3f} seconds")
print(f"   • Early stopping triggered: {'Yes' if mlp_model.n_iter_ < 500 else 'No'}")

# Compare with other models
print(f"\n6. MODEL COMPARISON SUMMARY:")
if 'accuracy' in globals():
    print(f"   • Logistic Regression: {accuracy*100:.1f}%")
if 'rf_accuracy' in globals():
    print(f"   • Random Forest: {rf_accuracy*100:.1f}%")
print(f"   • Neural Network: {nn_accuracy*100:.1f}%")

print("\n🎯 NEURAL NETWORK SUMMARY:")
print("The Neural Network model demonstrates strong pattern recognition capabilities")
print("with its multi-layer architecture. The model successfully learned complex")
print("relationships between move sequences and game outcomes.")

# Final comparison visualization
plt.figure(figsize=(12, 6))

# Subplot 1: All models comparison
plt.subplot(1, 2, 1)
model_names = []
model_accuracies = []
model_f1s = []

if 'accuracy' in globals() and 'y_pred' in globals():
    model_names.append('Logistic\nRegression')
    model_accuracies.append(accuracy * 100)
    model_f1s.append(f1_score(y_test, y_pred, average='weighted') * 100)

if 'rf_accuracy' in globals():
    model_names.append('Random\nForest')
    model_accuracies.append(rf_accuracy * 100)
    # Calculate RF F1 if rf_pred exists
    if 'rf_pred' in globals() and 'y_test_rf' in globals():
        model_f1s.append(f1_score(y_test_rf, rf_pred, average='weighted') * 100)
    else:
        model_f1s.append(0)

model_names.append('Neural\nNetwork')
model_accuracies.append(nn_accuracy * 100)
model_f1s.append(nn_f1 * 100)

x = np.arange(len(model_names))
width = 0.35

bars1 = plt.bar(x - width/2, model_accuracies, width, label='Accuracy', color='lightcoral', alpha=0.8)
bars2 = plt.bar(x + width/2, model_f1s, width, label='F1 Score', color='lightblue', alpha=0.8)

plt.xlabel('Model')
plt.ylabel('Score (%)')
plt.title('Neural Network vs Other Models', fontsize=14, fontweight='bold')
plt.xticks(x, model_names)
plt.legend()
plt.grid(axis='y', alpha=0.3)

for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                 f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')

# Subplot 2: Confidence comparison
plt.subplot(1, 2, 2)
models = ['Neural Network']
confidences = [nn_max_proba.mean()]

if 'y_pred_proba' in globals():
    models.insert(0, 'Logistic Regression')
    confidences.insert(0, np.max(y_pred_proba, axis=1).mean())

plt.bar(models, confidences, color=['lightcoral', 'royalblue'][:len(models)], alpha=0.8)
plt.ylabel('Average Confidence')
plt.title('Model Confidence Comparison', fontsize=14, fontweight='bold')
plt.ylim(0, 1)
plt.grid(axis='y', alpha=0.3)

for i, conf in enumerate(confidences):
    plt.text(i, conf + 0.01, f'{conf:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

# Save the model for later use
print("\n💾 Saving neural network model...")
nn_model_data = {
    'model': mlp_model,
    'scaler': scaler,
    'encoders': le_dict,
    'label_encoder_y': label_encoder_y,
    'feature_columns': move_columns,
    'accuracy': nn_accuracy
}
print("Model saved to nn_model_data dictionary")

